# **what?**
# This workflow will test a single test a given number of times to determine if it's flaky or not.  You can test with any supported OS/Python combination.

# **why?**
# Testing if a test is flaky and if a previously flaky test has been fixed.  This allows easy testing on supported python versions and OS combinations.

# **when?**
# This is triggered manually from dbt-core.

name: Flaky Tester

on:
  workflow_dispatch:
    inputs:
      branch:
        description: 'Branch to check out'
        type: string
        required: true
        default: 'main'
      test_path:
        description: 'Path to single test to run (ex: tests/functional/retry/test_retry.py::TestRetry::test_fail_fast)'
        type: string
        required: true
        default: 'tests/functional/...'
      python_version:
        description: 'Version of Python to Test Against'
        type: choice
        options:
          - '3.8'
          - '3.9'
          - '3.10'
          - '3.11'
      os:
        description: 'OS to run test in'
        type: choice
        options:
          - 'ubuntu-latest'
          - 'macos-latest'
          - 'windows-latest'
      num_runs:
        description: 'Max number of times to run the test'
        type: number
        required: true
        default: '100'

permissions: read-all

defaults:
  run:
    shell: bash

jobs:
  pytest:
    runs-on: ${{ inputs.os }}
    env:
      PYTEST_ADDOPTS: "-v --color=yes -n4 --csv integration_results.csv"
      DBT_TEST_USER_1: dbt_test_user_1
      DBT_TEST_USER_2: dbt_test_user_2
      DBT_TEST_USER_3: dbt_test_user_3

    steps:
      - name: "[DEBUG] Output Inputs"
        run: |
          echo "Branch: ${{ inputs.branch }}"
          echo "test_path: ${{ inputs.test_path }}"
          echo "python_version: ${{ inputs.python_version }}"
          echo "os: ${{ inputs.os }}"
          echo "num_runs: ${{ inputs.num_runs }}"

      - name: "Checkout code"
        uses: actions/checkout@v3
        with:
          ref: ${{ inputs.branch }}

      - name: "Setup Python"
        uses: actions/setup-python@v4
        with:
          python-version: "${{ inputs.python_version }}"

      - name: "Setup Dev Environment"
        run: make dev

      - name: "Set up postgres (linux)"
        if: inputs.os == 'ubuntu-latest'
        run: make setup-db

      - name: "Set up postgres (macos)"
        if: inputs.os == 'macos-latest'
        uses: ./.github/actions/setup-postgres-macos

      - name: "Set up postgres (windows)"
        if: inputs.os == 'windows-latest'
        uses: ./.github/actions/setup-postgres-windows

      - name: "Test Command"
        id: command
        run: |
          test_command="python -m pytest ${{ inputs.test_path }}"
          echo "test_command=$test_command" >> $GITHUB_OUTPUT

      - name: "Run test ${{ inputs.num_runs }} times"
        id: pytest
        run: |
          for ((i=1; i<=${{ inputs.num_runs }}; i++))
          do
            echo "Running pytest iteration $i..."
            pytest_output=$(${{ steps.command.outputs.test_command }})
            return_code=$?

            if [[ $return_code -eq 0 ]]; then
              success=$((success + 1))
              echo "Iteration $i: Success"
            else
              failure=$((failure + 1))
              echo "Iteration $i: Failure"
            fi

            echo "$pytest_output"
            echo
          done

          echo "Successful runs: $success"
          echo "Failed runs: $failure"
          echo "failure=$failure" >> $GITHUB_OUTPUT

      - name: "Check Failures for ${{ inputs.os }}/Python ${{ inputs.python_version }}"
        if: ${{ steps.pytest.outputs.failure }}
        run: |
            echo "${{ steps.pytest.outputs.failure }} of ${{ inputs.num_runs }} tests did not pass"
            exit 1
